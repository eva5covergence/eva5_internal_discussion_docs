{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final_GridSearch_S_E_EVA5_S5_v6_FineTune_LR_scheduler_final_S6_L1_BN.ipynb","provenance":[{"file_id":"1657lPmGc19o8V962MGS2gxHHyRH2UqvY","timestamp":1598530815782},{"file_id":"1S4x7aKoMf3FOcZjsvIOok7Kgebl1N6RQ","timestamp":1598502194415},{"file_id":"1OHGqLyD3drzJsEQZoKSzEWRi-iVFqU3x","timestamp":1598425777205},{"file_id":"11aO5MvPbTIH8XhaVGwT_Xyer_hNtCUQO","timestamp":1598378480510},{"file_id":"1dfMT9YubdT_ZyBEKRQlntPLCYW64cMj0","timestamp":1597953574625},{"file_id":"1iB6PmFLHbR97kKY_hQOCQ8hNKQIAxTaz","timestamp":1597950458466},{"file_id":"1EPKHLE64sTorpo_Lg_npxfZohd7RqMGB","timestamp":1597949817775},{"file_id":"1_EgbhPf92U_W8S7UzC38hxggS30Vq-BA","timestamp":1597949411678},{"file_id":"1yDfE0pIWjGkB51NqzhgYMW5dxh13R1Hc","timestamp":1597948798432},{"file_id":"1y3CmvuCn2PyWGtiALyHXJhlvPCc1gbKg","timestamp":1597948004452},{"file_id":"1zx12oDfnadaVjEwQfUtAwfCQTSqZxRwj","timestamp":1597946538285},{"file_id":"1aFgWmHNJoCyZ56zRvoE8xUdAe285aWmb","timestamp":1581394625836}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0MWDgf5dUEFk","colab_type":"text"},"source":["# FineTune_LR_scheduler - S5_v6"]},{"cell_type":"markdown","metadata":{"id":"1zyfvtCFICOU","colab_type":"text"},"source":["\n","# Target:\n","\n","1. FineTune LR scheduler. Set LR=0.1 as before but updated StepSize = 12 and Gamma = 0.2\n","\n","# Results:\n","\n","1. Parameters: 7,612\n","2. Best Train Accuracy: 99.41\n","3. Best Test Accuracy: 99.49\n","\n","# Analysis:\n","1. To get best combination values StepSize = 12 and Gamma =0.2, we tried many trails of these two values.\n","2. The intuition behind above values is, we observed the accuracy is gradually increasing till around 10 epochs and getting stall from there. So we would like to update LR around 10-12 epochs.\n","3. We tried with StepSize and Gamma combinations - (10, 0.1), (11, 0.1), (12, 0.1) But didn't help to get the target accuracy consistently at last few epochs.\n","4. So we thought to increase the speed a little bit after 10-12 epochs by updating gamma = 0.2 and tried these StepSize and Gamma combinations - (10, 0.2), (11, 0.2), (12, 0.2) And finaally Stepsize=12, Gamma=0.2 gave best consistency of >=99.4% in the last 3 epochs and hit maximum of 99.49% with less than 8000 parameters\n"]},{"cell_type":"markdown","metadata":{"id":"aO-7t1Y7-hV4","colab_type":"text"},"source":["# Import Libraries"]},{"cell_type":"code","metadata":{"id":"8kH16rnZ7wt_","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import time "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FkgEpHeg6fOH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1598543553776,"user_tz":-330,"elapsed":1448,"user":{"displayName":"naga pavankumar kalepu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyEBiz_-v88kOZHew5XXK9Wy_BbsB0w0O1YiddVA=s64","userId":"14463626350540278797"}},"outputId":"d2a5c92c-f82b-43c9-bd0b-479261cb8b2d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G77NEAIK3PaV","colab_type":"code","colab":{}},"source":["import logging\n","  \n","logger = logging.getLogger(\"\")\n","#logging.basicConfig(level=logging.DEBUG)\n","filename = '/content/drive/My Drive/Final_GridSearch_S_E_EVA5_S5_v6_FineTune_LR_scheduler_final_S6_L1_BN_'+time.ctime().replace(' ','_')+'.txt'\n","logging.basicConfig(level = logging.DEBUG, filename = filename)\n","# logger.debug('Loging %s lewel', 'DEBUG')\n","# logger.info('Loging %s lewel', 'INFO')\n","# logger.warning('Loging %s lewel', 'WARN')\n","# logger.error('Loging %s lewel', 'ERROR')\n","# logger.critical('Loging %s lewel', 'CRITICAL')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hmcHYOj6hKpj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1598543553777,"user_tz":-330,"elapsed":1436,"user":{"displayName":"naga pavankumar kalepu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyEBiz_-v88kOZHew5XXK9Wy_BbsB0w0O1YiddVA=s64","userId":"14463626350540278797"}},"outputId":"381295b2-1dc1-4817-9b47-3916f2ce653c"},"source":["time.ctime()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Thu Aug 27 15:52:33 2020'"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"7RlReRL6mDLk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1598543553778,"user_tz":-330,"elapsed":1431,"user":{"displayName":"naga pavankumar kalepu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyEBiz_-v88kOZHew5XXK9Wy_BbsB0w0O1YiddVA=s64","userId":"14463626350540278797"}},"outputId":"7678e948-81dc-4926-ee4a-1b9d54b61754"},"source":["time.ctime().replace(' ','_')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Thu_Aug_27_15:52:33_2020'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"ky3f_Odl-7um","colab_type":"text"},"source":["## Data Transformations\n","\n","We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise. \n"]},{"cell_type":"code","metadata":{"id":"YtssFUKb-jqx","colab_type":"code","colab":{}},"source":["train_transforms = transforms.Compose([\n","    transforms.RandomRotation((-7.0, 7.0), fill=(1,)),                                   \n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,))\n","])\n","\n","test_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,))\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oQciFYo2B1mO","colab_type":"text"},"source":["# Dataset and Creating Train/Test Split"]},{"cell_type":"code","metadata":{"id":"_4A84rlfDA23","colab_type":"code","colab":{}},"source":["train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n","test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qgldp_3-Dn0c","colab_type":"text"},"source":["# Dataloader Arguments & Test/Train Dataloaders\n"]},{"cell_type":"code","metadata":{"id":"C8OLDR79DrHG","colab_type":"code","colab":{}},"source":["SEED = 1\n","\n","# CUDA?\n","cuda = torch.cuda.is_available()\n","# logger.info(\"CUDA Available?\", cuda)\n","# logger.info(f\"CUDA Available? {cuda}\")\n","\n","# For reproducibility\n","torch.manual_seed(SEED)\n","\n","if cuda:\n","    torch.cuda.manual_seed(SEED)\n","\n","# dataloader arguments - something you'll fetch these from cmdprmt\n","dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n","\n","# train dataloader\n","train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n","\n","# test dataloader\n","test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ubQL3H6RJL3h","colab_type":"text"},"source":["# The model\n","Let's start with the model we first saw"]},{"cell_type":"code","metadata":{"id":"Ts6A8MUHg5sA","colab_type":"code","colab":{}},"source":["class BatchNorm(nn.BatchNorm2d):\n","    def __init__(self, num_features, eps=1e-05, momentum=0.1, weight=True, bias=True):\n","        super().__init__(num_features, eps=eps, momentum=momentum)\n","        self.weight.data.fill_(1.0)\n","        self.bias.data.fill_(0.0)\n","        self.weight.requires_grad = weight\n","        self.bias.requires_grad = bias\n","\n","\n","class GhostBatchNorm(BatchNorm):\n","    def __init__(self, num_features, num_splits, **kw):\n","        super().__init__(num_features, **kw)\n","        self.num_splits = num_splits\n","        self.register_buffer('running_mean', torch.zeros(num_features * self.num_splits))\n","        self.register_buffer('running_var', torch.ones(num_features * self.num_splits))\n","\n","    def train(self, mode=True):\n","        if (self.training is True) and (mode is False):  # lazily collate stats when we are going to use them\n","            self.running_mean = torch.mean(self.running_mean.view(self.num_splits, self.num_features), dim=0).repeat(\n","                self.num_splits)\n","            self.running_var = torch.mean(self.running_var.view(self.num_splits, self.num_features), dim=0).repeat(\n","                self.num_splits)\n","        return super().train(mode)\n","\n","    def forward(self, input):\n","        N, C, H, W = input.shape\n","        if self.training or not self.track_running_stats:\n","            return F.batch_norm(\n","                input.view(-1, C * self.num_splits, H, W), self.running_mean, self.running_var,\n","                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n","                True, self.momentum, self.eps).view(N, C, H, W)\n","        else:\n","            return F.batch_norm(\n","                input, self.running_mean[:self.num_features], self.running_var[:self.num_features],\n","                self.weight, self.bias, False, self.momentum, self.eps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7FXQlB9kH1ov","colab_type":"code","colab":{}},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # Input Block\n","        self.convblock1 = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(8)\n","        ) # output_size = 26\n","\n","        # CONVOLUTION BLOCK 1\n","        self.convblock2 = nn.Sequential(\n","            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(16)\n","        ) # output_size = 24\n","\n","        # TRANSITION BLOCK 1\n","        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12\n","        self.convblock3 = nn.Sequential(\n","            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(1, 1), padding=0, bias=False),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(8)\n","        ) # output_size = 12\n","\n","        # CONVOLUTION BLOCK 2\n","        self.convblock4 = nn.Sequential(\n","            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(16)\n","        ) # output_size = 10\n","        self.convblock5 = nn.Sequential(\n","            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(32)\n","        ) # output_size = 8\n","\n","        # OUTPUT BLOCK\n","        self.convblock6 = nn.Sequential(\n","            nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(10)\n","        ) # output_size = 8\n","        self.gap = nn.Sequential(\n","            nn.AvgPool2d(kernel_size=8)\n","        ) # output_size = 1\n","\n","    def forward(self, x):\n","        x = self.convblock1(x)\n","        x = self.convblock2(x)\n","        x = self.pool1(x)\n","        x = self.convblock3(x)\n","        x = self.convblock4(x)\n","        x = self.convblock5(x)\n","        x = self.convblock6(x)\n","        x = self.gap(x)\n","        x = x.view(-1, 10)\n","        return F.log_softmax(x, dim=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M3-vp8X9LCWo","colab_type":"text"},"source":["# Model Params\n","Can't emphasize on how important viewing Model Summary is. \n","Unfortunately, there is no in-built model visualizer, so we have to take external help"]},{"cell_type":"code","metadata":{"id":"5skB97zIJQQe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":621},"executionInfo":{"status":"ok","timestamp":1598543560636,"user_tz":-330,"elapsed":8251,"user":{"displayName":"naga pavankumar kalepu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyEBiz_-v88kOZHew5XXK9Wy_BbsB0w0O1YiddVA=s64","userId":"14463626350540278797"}},"outputId":"0decdeeb-197c-4865-e458-90f69a19bc7c"},"source":["!pip install torchsummary\n","from torchsummary import summary\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","# logger.info(device)\n","logger.info(f\"Device : {device}\")\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 8, 26, 26]              72\n","              ReLU-2            [-1, 8, 26, 26]               0\n","       BatchNorm2d-3            [-1, 8, 26, 26]              16\n","            Conv2d-4           [-1, 16, 24, 24]           1,152\n","              ReLU-5           [-1, 16, 24, 24]               0\n","       BatchNorm2d-6           [-1, 16, 24, 24]              32\n","         MaxPool2d-7           [-1, 16, 12, 12]               0\n","            Conv2d-8            [-1, 8, 12, 12]             128\n","              ReLU-9            [-1, 8, 12, 12]               0\n","      BatchNorm2d-10            [-1, 8, 12, 12]              16\n","           Conv2d-11           [-1, 16, 10, 10]           1,152\n","             ReLU-12           [-1, 16, 10, 10]               0\n","      BatchNorm2d-13           [-1, 16, 10, 10]              32\n","           Conv2d-14             [-1, 32, 8, 8]           4,608\n","             ReLU-15             [-1, 32, 8, 8]               0\n","      BatchNorm2d-16             [-1, 32, 8, 8]              64\n","           Conv2d-17             [-1, 10, 8, 8]             320\n","             ReLU-18             [-1, 10, 8, 8]               0\n","      BatchNorm2d-19             [-1, 10, 8, 8]              20\n","        AvgPool2d-20             [-1, 10, 1, 1]               0\n","================================================================\n","Total params: 7,612\n","Trainable params: 7,612\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.48\n","Params size (MB): 0.03\n","Estimated Total Size (MB): 0.51\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fT_FHOFBami1","colab_type":"code","colab":{}},"source":["# for i in model.parameters():\n","#   logger.info(i)\n","#   break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1__x_SbrL7z3","colab_type":"text"},"source":["# Training and Testing\n","\n","Looking at logs can be boring, so we'll introduce **tqdm** progressbar to get cooler logs. \n","\n","Let's write train and test functions"]},{"cell_type":"code","metadata":{"id":"lV8c9I01L9M_","colab_type":"code","colab":{}},"source":["def get_current_train_acc(model, train_loader):\n","  model.eval()\n","  train_loss = 0\n","  correct = 0\n","  with torch.no_grad():\n","      for data, target in train_loader:\n","          data, target = data.to(device), target.to(device)\n","          output = model(data)\n","          train_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","          pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","          correct += pred.eq(target.view_as(pred)).sum().item()\n","  train_loss /= len(train_loader.dataset)\n","\n","  logger.info('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","      train_loss, correct, len(train_loader.dataset),\n","      100. * correct / len(train_loader.dataset)))\n","  \n","  train_acc = 100. * correct / len(train_loader.dataset)\n","  return train_acc, train_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fbkF2nN_LYIb","colab_type":"code","colab":{}},"source":["from tqdm import tqdm\n","\n","# train_losses = []\n","# test_losses = []\n","# train_acc = []\n","# test_acc = []\n","\n","def train(model, device, train_loader, optimizer, lambda_l1=0, train_acc=[], train_losses=[]):\n","  model.train()\n","  pbar = tqdm(train_loader)\n","  correct = 0\n","  processed = 0\n","  for batch_idx, (data, target) in enumerate(pbar):\n","    # get samples\n","    data, target = data.to(device), target.to(device)\n","\n","    # Init\n","    optimizer.zero_grad()\n","    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n","    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n","\n","    # Predict\n","    y_pred = model(data)\n","\n","    # Calculate loss\n","    loss = F.nll_loss(y_pred, target)\n","    #train_losses.append(loss)\n","\n","    # L1 regularisation\n","\n","    l1 = 0\n","    for p in model.parameters():\n","      l1 += p.abs().sum()\n","    loss += lambda_l1 * l1\n","\n","    # Backpropagation\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Update pbar-tqdm\n","    \n","    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","    correct += pred.eq(target.view_as(pred)).sum().item()\n","    processed += len(data)\n","\n","    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Current_train_batch_accuracy={100*correct/processed:0.2f}')\n","  current_train_acc, current_train_loss = get_current_train_acc(model, train_loader)\n","  train_acc.append(current_train_acc)\n","  train_losses.append(current_train_loss)\n","  return train_acc, train_losses\n","\n","def test(model, device, test_loader, test_acc=[], test_losses=[]):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    test_losses.append(test_loss)\n","\n","    logger.info('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","    \n","    test_acc.append(100. * correct / len(test_loader.dataset))\n","    return test_acc, test_losses\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"drokW8wWODKq","colab_type":"text"},"source":["# Let's Train and test our model"]},{"cell_type":"code","metadata":{"id":"P23-o-NBFa0E","colab_type":"code","colab":{}},"source":["# def save_best_model(epochs, model, device, train_loader, optimizer, lambda_l1=0.0, scheduler):\n","#   for epoch in range(EPOCHS):\n","#     logger.info(f\" ***** EPOCH:{epoch} ***** \")\n","#     train(model, device, train_loader, optimizer, epoch, lambda_l1)\n","#     scheduler.step()\n","#     test(model, device, test_loader)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8p4iyxo0IdK4","colab_type":"code","colab":{}},"source":["def get_best_train_test_acc(train_acc=[], test_acc=[]):\n","  \"\"\"\n","  Example:\n","  train_acc_1=[96.5,98.7,99.2,99.3];test_acc_1=[97.2,98.5, 99.25, 99.2]\n","  assert get_best_train_test_acc(train_acc_1, test_acc_1)==(99.2, 99.25)\n","  \"\"\"\n","  tr_te_acc_pairs = list(zip(train_acc, test_acc))\n","  tr_te_acc_pairs_original = tr_te_acc_pairs[:]\n","  tr_te_acc_pairs.sort(key = lambda x: x[1], reverse=True)\n","  for tr_acc, te_acc in tr_te_acc_pairs:\n","    if tr_acc > te_acc and tr_acc - te_acc >= 1:\n","      tr_te_acc_pairs.remove((tr_acc, te_acc))\n","  return tr_te_acc_pairs[0], tr_te_acc_pairs_original.index(tr_te_acc_pairs[0])+1\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMo6bSgKYFJS","colab_type":"code","colab":{}},"source":["def save_model(model, PATH='./test_model.pickle'):\n","  \"\"\"\n","   Save trained model at given PATH\n","  \"\"\"\n","  torch.save(model.state_dict(), PATH)\n","  logger.info(f\"Model saved at {PATH}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iewpQKUSUUb-","colab_type":"code","colab":{}},"source":["train_acc_1=[96.5,98.7,99.2,99.3];test_acc_1=[97.2,98.5, 99.25, 99.2]\n","get_best_train_test_acc(train_acc_1, test_acc_1)\n","\n","assert get_best_train_test_acc(train_acc_1, test_acc_1)==((99.2, 99.25),3)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FwZLjJpdcjv2","colab_type":"code","colab":{}},"source":["def fit_model(epochs, model, device, train_loader, test_loader, optimizer, lambda_l1, scheduler):\n","  train_acc = []\n","  train_losses = []\n","  test_acc = []\n","  test_losses = []\n","  for epoch in range(EPOCHS):\n","    logger.info(f\"[EPOCH:{epoch}]\")\n","    train_acc, train_losses = train(model, device, train_loader, optimizer, lambda_l1, train_acc, train_losses)\n","    scheduler.step()\n","    test_acc, test_losses = test(model, device, test_loader, test_acc, test_losses)\n","  return train_acc, train_losses, test_acc, test_losses\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mE7n4kFFydW1","colab_type":"code","colab":{}},"source":["      # train_acc, train_losses, test_acc, test_losses = fit_model(epochs, model, device, train_loader, test_loader, optimizer, para, scheduler)\n","      # (best_train_acc, best_test_acc), epoch = get_best_train_test_acc(train_acc, test_acc)\n","      # logger.info(f\"For L1 lambda parameter {para} Best train Accuracy {best_train_acc}% and Best Test Accuracy {best_test_acc}% at Epoch {epoch}\")\n","      # all_lambdal1_train_test_acc_from_best_epoch.append((best_train_acc, best_test_acc, para))\n","      # temp_best_train_acc_list.append(best_train_acc)\n","      # temp_best_test_acc_list.append(best_test_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljNM8Om6vqjo","colab_type":"code","colab":{}},"source":["def best_tr_te_acc_from_epoch(epochs, model, device, train_loader, test_loader, optimizer, lambda_l1=0, lambda_l2=0, scheduler=None):\n","  temp_best_train_acc_list = []\n","  temp_best_test_acc_list = []\n","  all_lambdal1_train_test_acc_from_best_epoch =[]\n","  train_acc, train_losses, test_acc, test_losses = fit_model(epochs, model, device, train_loader, test_loader, optimizer, lambda_l1=lambda_l1, scheduler=scheduler)\n","  (best_train_acc, best_test_acc), epoch = get_best_train_test_acc(train_acc, test_acc)\n","  logger.info(f\"\\n===================> For L1 lambda parameter {lambda_l1}, For L2 lambda parameter {lambda_l2}, Best train Accuracy {best_train_acc}% and Best Test Accuracy {best_test_acc}% at Epoch {epoch} <===================\\n\")\n","  all_lambdal1_train_test_acc_from_best_epoch.append((best_train_acc, best_test_acc, lambda_l1, lambda_l2 ))\n","  temp_best_train_acc_list.append(best_train_acc)\n","  temp_best_test_acc_list.append(best_test_acc)\n","  return temp_best_train_acc_list, temp_best_test_acc_list, all_lambdal1_train_test_acc_from_best_epoch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9FViXBzUdwDl","colab_type":"code","colab":{}},"source":["def my_grid_search(epochs, model, device, train_loader, test_loader, optimizer, scheduler, lambda_l1_range = [], lambda_l2_range = [], size = 20, l1_l2_trails=0):\n","  best_lambdal1_train_acc = 0.0\n","  best_lambdal1_test_acc = 0.0\n","  best_lambdal1 = 0.0\n","  all_lambdal1_train_test_acc_from_best_epoch = []\n"," \n","\n","  if lambda_l1_range and lambda_l2_range:\n","    if lambda_l1_range[0]>lambda_l1_range[1] or lambda_l2_range[0]>lambda_l2_range[1]:\n","      raise Exception(\"It should be => min<max\")\n","    options_l1 = np.random.uniform(low=lambda_l1_range[0], high=lambda_l1_range[1], size=size)\n","    options_l2 = np.random.uniform(low=lambda_l2_range[0], high=lambda_l2_range[1], size=size)\n","    for i in range(l1_l2_trails):\n","      l1_value = random.choice(options_l1)\n","      l2_value = random.choice(options_l2)\n","      logger.info(f\"\\n L1&L2 Trail:{i+1} - Model is getting trained with L1 regularisation parameter {l1_value} and L2 regularisation parameter {l2_value}\\n\")\n","      model =  Net().to(device)\n","      optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=l2_value)\n","      scheduler = StepLR(optimizer, step_size=12, gamma=0.2)\n","      temp_best_train_acc_list, temp_best_test_acc_list, all_lambdal1_train_test_acc_from_best_epoch = best_tr_te_acc_from_epoch(epochs, model, device, train_loader, test_loader, optimizer, lambda_l1=l1_value, lambda_l2=l2_value, scheduler=scheduler)\n","    \n","    (best_para_train_acc, best_para_test_acc), idx = get_best_train_test_acc(temp_best_train_acc_list, temp_best_test_acc_list)\n","    idx -= 1\n","    final_best_train_acc, final_best_test_acc, final_best_lambda_l1, final_best_lambda_l2 = all_lambdal1_train_test_acc_from_best_epoch[idx]\n","    logger.info(f\"\\n===================> final_best_train_acc: {final_best_train_acc}, final_best_test_acc: {final_best_test_acc}, final_best_lambda_l1: {final_best_lambda_l1} , final_best_lambda_l2: {final_best_lambda_l2} <===================\\n\")\n","    return final_best_train_acc, final_best_test_acc, final_best_lambda_l1, final_best_lambda_l2\n","\n","\n","  elif lambda_l1_range:\n","    if lambda_l1_range[0]>lambda_l1_range[1]:\n","      raise Exception(\"It should be => lambda_l1_range[0]<lambda_l1_range[1]\")\n","    options = np.random.uniform(low=lambda_l1_range[0], high=lambda_l1_range[1], size=size)\n","    for i, para in enumerate(options):\n","      logger.info(f\"\\n L1 Trail:{i+1} - Model is getting trained with L1 regularisation parameter {para}\\n\")\n","      model =  Net().to(device)\n","      optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","      scheduler = StepLR(optimizer, step_size=12, gamma=0.2)\n","      temp_best_train_acc_list, temp_best_test_acc_list, all_lambdal1_train_test_acc_from_best_epoch = best_tr_te_acc_from_epoch(epochs, model, device, train_loader, test_loader, optimizer, lambda_l1=para, lambda_l2=0, scheduler=scheduler)\n","    (best_para_train_acc, best_para_test_acc), idx = get_best_train_test_acc(temp_best_train_acc_list, temp_best_test_acc_list)\n","    idx -= 1\n","    final_best_train_acc, final_best_test_acc, final_best_lambda_l1, final_best_lambda_l2 = all_lambdal1_train_test_acc_from_best_epoch[idx]\n","    logger.info(f\"\\n===================> final_best_train_acc: {final_best_train_acc}, final_best_test_acc: {final_best_test_acc}, final_best_lambda_l1: {final_best_lambda_l1} <===================\\n\")\n","    return final_best_train_acc, final_best_test_acc, final_best_lambda_l1, final_best_lambda_l2\n","\n","  elif lambda_l2_range:\n","    if lambda_l2_range[0]>lambda_l2_range[1]:\n","      raise Exception(\"It should be => lambda_l2_range[0]<lambda_l2_range[1]\")\n","    options = np.random.uniform(low=lambda_l2_range[0], high=lambda_l2_range[1], size=size)\n","    for i, para in enumerate(options):\n","      logger.info(f\"\\n L2 Trail:{i+1} - Model is getting trained with L2 regularisation parameter {para}\\n\")\n","      model =  Net().to(device)\n","      optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=para)\n","      scheduler = StepLR(optimizer, step_size=12, gamma=0.2)\n","      temp_best_train_acc_list, temp_best_test_acc_list, all_lambdal1_train_test_acc_from_best_epoch = best_tr_te_acc_from_epoch(epochs, model, device, train_loader, test_loader, optimizer=optimizer, lambda_l1=0, lambda_l2=para, scheduler=scheduler)\n","    (best_para_train_acc, best_para_test_acc), idx = get_best_train_test_acc(temp_best_train_acc_list, temp_best_test_acc_list)\n","    idx -= 1\n","    final_best_train_acc, final_best_test_acc, final_best_lambda_l1, final_best_lambda_l2 = all_lambdal1_train_test_acc_from_best_epoch[idx]\n","    logger.info(f\"\\n===================> final_best_train_acc: {final_best_train_acc}, final_best_test_acc: {final_best_test_acc}, final_best_lambda_l2: {final_best_lambda_l2} <===================\\n\")\n","    return final_best_train_acc, final_best_test_acc, final_best_lambda_l1, final_best_lambda_l2\n","\n","  else:\n","    raise Exception(\"Select at least one parameter to search its mathematical space\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xMCFxeAKOB53","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"6a5167c5-9d52-44de-8e10-bc49681d2151"},"source":["model =  Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","EPOCHS = 15\n","scheduler = StepLR(optimizer, step_size=12, gamma=0.2)\n","# lambda_l1=0\n","l1_l2_trails=50\n","\n","# para_grid_lambda = [[0,0.1],[0,0.01],[0,0.001],[0,0.0001]]\n","para_grid_lambda = [[0,0.0001], [0,0.001], [0,0.01],[0,0.1]]\n","# para_grid_lambda = [[0,0.0001], [0,0.1]]\n","results_lambda_l1 = []\n","results_lambda_l2 = []\n","results_lambda_l1_l2 = []\n","size = 20 # Number of random choices in the given range\n","\n","\n","## L1 regularisation hyper parameter search\n","\n","for para_range in para_grid_lambda:\n","  logger.info(f\"\\n===================> Started - Trail on L1 reg parameters range - {para_range}, Number of trails - {size}, Number of Epochs - {EPOCHS} <===================\\n\")\n","  final_best_train_acc, final_best_test_acc, final_best_lambda_l1, final_best_lambda_l2 = my_grid_search(EPOCHS, model, device, train_loader, test_loader, optimizer, scheduler, lambda_l1_range = para_range, lambda_l2_range = [], size = size)\n","  results_lambda_l1.append((final_best_train_acc, final_best_test_acc, final_best_lambda_l1, final_best_lambda_l2))\n","  logger.info(f\"\\n===================> current results_lambda_l1 - {results_lambda_l1} <===================\\n\")\n","  logger.info(f\"\\n===================> Completed - Trail on L1 reg parameters range - {para_range} <===================\\n\")\n","\n","logger.info(f\"\\n===================> L1 - Results of Coarse/finer grid search in various ranges - {para_grid_lambda}<===================\\n\")\n","for final_best_train_acc, final_best_test_acc, final_best_lambda_l1, final_best_lambda_l2 in results_lambda_l1:\n","  logger.info(f\"L1 reg parameter: {final_best_lambda_l1}, L2 reg parameter: {final_best_lambda_l2}, Train_acc: {final_best_train_acc}, Test_acc: {final_best_test_acc}\")\n","\n","# ## L2 regularisation hyper parameter search\n","\n","# for para_range in para_grid_lambda:\n","#   logger.info(f\"\\n===================> Started - Trail on L2 reg parameters range - {para_range}, Number of trails - {size}, Number of Epochs - {EPOCHS}<===================\\n\")\n","#   final_best_train_acc, final_best_test_acc, final_best_lambda_l1, final_best_lambda_l2 = my_grid_search(EPOCHS, model, device, train_loader, test_loader, optimizer, scheduler, lambda_l1_range = [], lambda_l2_range = para_range, size = size)\n","#   results_lambda_l2.append((final_best_train_acc, final_best_test_acc, final_best_lambda_l1, final_best_lambda_l2))\n","#   logger.info(f\"\\n===================> current results_lambda_l2 - {results_lambda_l2} <===================\\n\")\n","#   logger.info(f\"\\n===================> Completed - Trail on L2 reg parameters range - {para_range} <===================\\n\")\n","\n","# logger.info(f\"\\n===================> L2 - Results of Coarse/finer grid search in various ranges - {para_grid_lambda}<===================\\n\")\n","# for final_best_train_acc, final_best_test_acc, final_best_lambda_l1, final_best_lambda_l2 in results_lambda_l2:\n","#   logger.info(f\"L1 reg parameter: {final_best_lambda_l1}, L2 reg parameter: {final_best_lambda_l2}, Train_acc: {final_best_train_acc}, Test_acc: {final_best_test_acc}\")\n","\n","# ## L1&L2 regularisation hyper parameter search\n","\n","# # l1 and l2 reg paras in same range given but can be given different ranges by writing little more sophisticated logic\n","# for para_range in para_grid_lambda:\n","#   logger.info(f\"\\n===================> Started - Trail on L1 & L2 reg parameters range - {para_range}, Number of para_ranges - {size}, , Number of trails per para_range - {l1_l2_trails}, Number of Epochs - {EPOCHS}<===================\\n\")\n","#   final_best_train_acc, final_best_test_acc, final_best_lambda_l1, final_best_lambda_l2 = my_grid_search(EPOCHS, model, device, train_loader, test_loader, optimizer, scheduler, lambda_l1_range = para_range, lambda_l2_range = para_range, size = size, l1_l2_trails=l1_l2_trails)\n","#   results_lambda_l1_l2.append((final_best_train_acc, final_best_test_acc, final_best_lambda_l1, final_best_lambda_l2))\n","#   logger.info(f\"\\n===================> current results_lambda_l1_l2 - {results_lambda_l1_l2} <===================\\n\")\n","#   logger.info(f\"\\n===================> Completed - Trail on L1 and L2 reg parameters range - {para_range} <===================\\n\")\n","\n","# logger.info(f\"\\n===================> L1 & L2 - Results of Coarse/finer grid search in various ranges - {para_grid_lambda} <===================\\n\")\n","# for final_best_train_acc, final_best_test_acc, final_best_lambda_l1, final_best_lambda_l2 in results_lambda_l1_l2:\n","#   logger.info(f\"L1 reg parameter: {final_best_lambda_l1}, L2 reg parameter: {final_best_lambda_l2}, Train_acc: {final_best_train_acc}, Test_acc: {final_best_test_acc}\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loss=0.12899385392665863 Batch_id=468 Current_train_batch_accuracy=94.00: 100%|██████████| 469/469 [00:14<00:00, 31.27it/s]\n","Loss=0.11819936335086823 Batch_id=468 Current_train_batch_accuracy=97.92: 100%|██████████| 469/469 [00:14<00:00, 31.53it/s]\n","Loss=0.12428629398345947 Batch_id=468 Current_train_batch_accuracy=98.31: 100%|██████████| 469/469 [00:14<00:00, 31.65it/s]\n","Loss=0.09569384157657623 Batch_id=468 Current_train_batch_accuracy=98.42: 100%|██████████| 469/469 [00:14<00:00, 31.46it/s]\n","Loss=0.1254483014345169 Batch_id=468 Current_train_batch_accuracy=98.47: 100%|██████████| 469/469 [00:14<00:00, 31.45it/s]\n","Loss=0.08706681430339813 Batch_id=468 Current_train_batch_accuracy=98.56: 100%|██████████| 469/469 [00:14<00:00, 31.69it/s]\n","Loss=0.1018138974905014 Batch_id=468 Current_train_batch_accuracy=98.65: 100%|██████████| 469/469 [00:14<00:00, 31.34it/s]\n","Loss=0.0737680196762085 Batch_id=468 Current_train_batch_accuracy=98.66: 100%|██████████| 469/469 [00:14<00:00, 31.36it/s]\n","Loss=0.1044887825846672 Batch_id=468 Current_train_batch_accuracy=98.76: 100%|██████████| 469/469 [00:14<00:00, 31.37it/s]\n","Loss=0.0960947573184967 Batch_id=468 Current_train_batch_accuracy=98.77: 100%|██████████| 469/469 [00:14<00:00, 31.90it/s]\n","Loss=0.09928031265735626 Batch_id=468 Current_train_batch_accuracy=98.79: 100%|██████████| 469/469 [00:14<00:00, 32.00it/s]\n","Loss=0.11418326199054718 Batch_id=468 Current_train_batch_accuracy=98.76: 100%|██████████| 469/469 [00:14<00:00, 31.82it/s]\n","Loss=0.07967740297317505 Batch_id=468 Current_train_batch_accuracy=99.21: 100%|██████████| 469/469 [00:14<00:00, 32.66it/s]\n","Loss=0.14698246121406555 Batch_id=468 Current_train_batch_accuracy=99.31: 100%|██████████| 469/469 [00:14<00:00, 32.53it/s]\n","Loss=0.05912402272224426 Batch_id=468 Current_train_batch_accuracy=99.33: 100%|██████████| 469/469 [00:14<00:00, 32.28it/s]\n","Loss=0.14087355136871338 Batch_id=468 Current_train_batch_accuracy=93.59: 100%|██████████| 469/469 [00:14<00:00, 32.26it/s]\n","Loss=0.07513612508773804 Batch_id=468 Current_train_batch_accuracy=97.89: 100%|██████████| 469/469 [00:14<00:00, 31.58it/s]\n","Loss=0.057402826845645905 Batch_id=468 Current_train_batch_accuracy=98.36: 100%|██████████| 469/469 [00:14<00:00, 31.59it/s]\n","Loss=0.06607360392808914 Batch_id=468 Current_train_batch_accuracy=98.46: 100%|██████████| 469/469 [00:14<00:00, 32.41it/s]\n","Loss=0.05662398412823677 Batch_id=468 Current_train_batch_accuracy=98.65: 100%|██████████| 469/469 [00:14<00:00, 32.88it/s]\n","Loss=0.05441301316022873 Batch_id=468 Current_train_batch_accuracy=98.74: 100%|██████████| 469/469 [00:14<00:00, 31.76it/s]\n","Loss=0.12726181745529175 Batch_id=468 Current_train_batch_accuracy=98.78: 100%|██████████| 469/469 [00:14<00:00, 32.18it/s]\n","Loss=0.06195344030857086 Batch_id=468 Current_train_batch_accuracy=98.78: 100%|██████████| 469/469 [00:14<00:00, 32.23it/s]\n","Loss=0.04053204506635666 Batch_id=468 Current_train_batch_accuracy=98.87: 100%|██████████| 469/469 [00:14<00:00, 32.37it/s]\n","Loss=0.03821466863155365 Batch_id=468 Current_train_batch_accuracy=98.96: 100%|██████████| 469/469 [00:14<00:00, 32.36it/s]\n","Loss=0.04000892862677574 Batch_id=468 Current_train_batch_accuracy=99.03: 100%|██████████| 469/469 [00:14<00:00, 32.15it/s]\n","Loss=0.04338154196739197 Batch_id=468 Current_train_batch_accuracy=98.93: 100%|██████████| 469/469 [00:14<00:00, 31.95it/s]\n","Loss=0.05549437552690506 Batch_id=468 Current_train_batch_accuracy=99.28: 100%|██████████| 469/469 [00:14<00:00, 32.30it/s]\n","Loss=0.04691369831562042 Batch_id=468 Current_train_batch_accuracy=99.44: 100%|██████████| 469/469 [00:14<00:00, 32.43it/s]\n","Loss=0.03210501745343208 Batch_id=468 Current_train_batch_accuracy=99.36: 100%|██████████| 469/469 [00:14<00:00, 32.01it/s]\n","Loss=0.1432042419910431 Batch_id=468 Current_train_batch_accuracy=94.06: 100%|██████████| 469/469 [00:14<00:00, 32.01it/s]\n","Loss=0.0960926041007042 Batch_id=468 Current_train_batch_accuracy=98.00: 100%|██████████| 469/469 [00:14<00:00, 32.33it/s]\n","Loss=0.18325373530387878 Batch_id=468 Current_train_batch_accuracy=98.28: 100%|██████████| 469/469 [00:14<00:00, 32.41it/s]\n","Loss=0.08076807111501694 Batch_id=468 Current_train_batch_accuracy=98.48: 100%|██████████| 469/469 [00:14<00:00, 32.34it/s]\n","Loss=0.07596690952777863 Batch_id=468 Current_train_batch_accuracy=98.59: 100%|██████████| 469/469 [00:14<00:00, 32.36it/s]\n","Loss=0.08143950998783112 Batch_id=468 Current_train_batch_accuracy=98.66: 100%|██████████| 469/469 [00:14<00:00, 31.91it/s]\n","Loss=0.09490758180618286 Batch_id=468 Current_train_batch_accuracy=98.66: 100%|██████████| 469/469 [00:14<00:00, 32.03it/s]\n","Loss=0.09460514038801193 Batch_id=468 Current_train_batch_accuracy=98.66: 100%|██████████| 469/469 [00:14<00:00, 32.27it/s]\n","Loss=0.12492422759532928 Batch_id=468 Current_train_batch_accuracy=98.77: 100%|██████████| 469/469 [00:14<00:00, 32.72it/s]\n","Loss=0.09444308280944824 Batch_id=468 Current_train_batch_accuracy=98.71: 100%|██████████| 469/469 [00:14<00:00, 32.25it/s]\n","Loss=0.09489805996417999 Batch_id=468 Current_train_batch_accuracy=98.76: 100%|██████████| 469/469 [00:14<00:00, 31.60it/s]\n","Loss=0.06711484491825104 Batch_id=468 Current_train_batch_accuracy=98.81: 100%|██████████| 469/469 [00:14<00:00, 31.59it/s]\n","Loss=0.10211820900440216 Batch_id=468 Current_train_batch_accuracy=99.17: 100%|██████████| 469/469 [00:14<00:00, 32.09it/s]\n","Loss=0.06445928663015366 Batch_id=468 Current_train_batch_accuracy=99.32: 100%|██████████| 469/469 [00:14<00:00, 32.46it/s]\n","Loss=0.06113351136445999 Batch_id=468 Current_train_batch_accuracy=99.31: 100%|██████████| 469/469 [00:14<00:00, 32.34it/s]\n","Loss=0.18868161737918854 Batch_id=468 Current_train_batch_accuracy=93.60: 100%|██████████| 469/469 [00:14<00:00, 32.36it/s]\n","Loss=0.13280433416366577 Batch_id=468 Current_train_batch_accuracy=97.74: 100%|██████████| 469/469 [00:14<00:00, 32.47it/s]\n","Loss=0.12690205872058868 Batch_id=468 Current_train_batch_accuracy=98.08: 100%|██████████| 469/469 [00:14<00:00, 32.50it/s]\n","Loss=0.11787500977516174 Batch_id=468 Current_train_batch_accuracy=98.31: 100%|██████████| 469/469 [00:14<00:00, 32.44it/s]\n","Loss=0.09175001084804535 Batch_id=468 Current_train_batch_accuracy=98.43: 100%|██████████| 469/469 [00:14<00:00, 32.38it/s]\n","Loss=0.11794725060462952 Batch_id=468 Current_train_batch_accuracy=98.55: 100%|██████████| 469/469 [00:14<00:00, 32.30it/s]\n","Loss=0.10896922647953033 Batch_id=468 Current_train_batch_accuracy=98.51: 100%|██████████| 469/469 [00:14<00:00, 32.55it/s]\n","Loss=0.09282730519771576 Batch_id=468 Current_train_batch_accuracy=98.62: 100%|██████████| 469/469 [00:14<00:00, 32.25it/s]\n","Loss=0.0884542465209961 Batch_id=468 Current_train_batch_accuracy=98.71: 100%|██████████| 469/469 [00:14<00:00, 31.76it/s]\n","Loss=0.09058757871389389 Batch_id=468 Current_train_batch_accuracy=98.66: 100%|██████████| 469/469 [00:14<00:00, 32.16it/s]\n","Loss=0.12048055231571198 Batch_id=468 Current_train_batch_accuracy=98.66: 100%|██████████| 469/469 [00:14<00:00, 32.25it/s]\n","Loss=0.10965500771999359 Batch_id=468 Current_train_batch_accuracy=98.72: 100%|██████████| 469/469 [00:14<00:00, 31.96it/s]\n","Loss=0.06403367221355438 Batch_id=468 Current_train_batch_accuracy=99.14: 100%|██████████| 469/469 [00:14<00:00, 32.72it/s]\n","Loss=0.14821267127990723 Batch_id=468 Current_train_batch_accuracy=99.25: 100%|██████████| 469/469 [00:14<00:00, 32.59it/s]\n","Loss=0.08099132031202316 Batch_id=468 Current_train_batch_accuracy=99.29: 100%|██████████| 469/469 [00:14<00:00, 32.53it/s]\n","Loss=0.1395258903503418 Batch_id=468 Current_train_batch_accuracy=93.40: 100%|██████████| 469/469 [00:14<00:00, 32.69it/s]\n","Loss=0.10277886688709259 Batch_id=468 Current_train_batch_accuracy=97.86: 100%|██████████| 469/469 [00:14<00:00, 32.83it/s]\n","Loss=0.19596432149410248 Batch_id=468 Current_train_batch_accuracy=98.13: 100%|██████████| 469/469 [00:14<00:00, 31.88it/s]\n","Loss=0.08559788763523102 Batch_id=468 Current_train_batch_accuracy=98.27: 100%|██████████| 469/469 [00:14<00:00, 32.04it/s]\n","Loss=0.16399255394935608 Batch_id=468 Current_train_batch_accuracy=98.52: 100%|██████████| 469/469 [00:14<00:00, 33.02it/s]\n","Loss=0.12717458605766296 Batch_id=468 Current_train_batch_accuracy=98.58: 100%|██████████| 469/469 [00:14<00:00, 32.84it/s]\n","Loss=0.07220102846622467 Batch_id=468 Current_train_batch_accuracy=98.59: 100%|██████████| 469/469 [00:14<00:00, 31.81it/s]\n","Loss=0.08892925083637238 Batch_id=468 Current_train_batch_accuracy=98.68: 100%|██████████| 469/469 [00:15<00:00, 30.56it/s]\n","Loss=0.05844450742006302 Batch_id=468 Current_train_batch_accuracy=98.76: 100%|██████████| 469/469 [00:15<00:00, 30.64it/s]\n","Loss=0.11504761129617691 Batch_id=468 Current_train_batch_accuracy=98.75: 100%|██████████| 469/469 [00:14<00:00, 31.56it/s]\n","Loss=0.05747384577989578 Batch_id=468 Current_train_batch_accuracy=98.82: 100%|██████████| 469/469 [00:14<00:00, 31.46it/s]\n","Loss=0.062139227986335754 Batch_id=468 Current_train_batch_accuracy=98.79: 100%|██████████| 469/469 [00:14<00:00, 31.31it/s]\n","Loss=0.06951670348644257 Batch_id=468 Current_train_batch_accuracy=99.18: 100%|██████████| 469/469 [00:14<00:00, 31.71it/s]\n","Loss=0.056278590112924576 Batch_id=468 Current_train_batch_accuracy=99.28: 100%|██████████| 469/469 [00:14<00:00, 32.03it/s]\n","Loss=0.08881857991218567 Batch_id=468 Current_train_batch_accuracy=99.30: 100%|██████████| 469/469 [00:14<00:00, 31.74it/s]\n","Loss=0.11833001673221588 Batch_id=468 Current_train_batch_accuracy=94.22: 100%|██████████| 469/469 [00:14<00:00, 31.79it/s]\n","Loss=0.1130613163113594 Batch_id=468 Current_train_batch_accuracy=97.95: 100%|██████████| 469/469 [00:14<00:00, 31.82it/s]\n","Loss=0.07816348224878311 Batch_id=468 Current_train_batch_accuracy=98.35: 100%|██████████| 469/469 [00:14<00:00, 31.57it/s]\n","Loss=0.08966511487960815 Batch_id=468 Current_train_batch_accuracy=98.54: 100%|██████████| 469/469 [00:14<00:00, 31.96it/s]\n","Loss=0.055611543357372284 Batch_id=468 Current_train_batch_accuracy=98.67: 100%|██████████| 469/469 [00:14<00:00, 31.63it/s]\n","Loss=0.11361868679523468 Batch_id=468 Current_train_batch_accuracy=98.67: 100%|██████████| 469/469 [00:14<00:00, 31.67it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Ojw9VJdwISTh","colab_type":"code","colab":{}},"source":["# save_model(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mrwcQ7TBjYen","colab_type":"code","colab":{}},"source":["# ## Load the model\n","# model_test = Net().to(device)\n","# PATH='./test_model.pickle'\n","# model_test.load_state_dict(torch.load(PATH))\n","# test(model_test, device, test_loader)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"87RaqGSEOWDe","colab_type":"code","colab":{}},"source":["# fig, axs = plt.subplots(2,2,figsize=(15,10))\n","# axs[0, 0].plot(train_losses)\n","# axs[0, 0].set_title(\"Training Loss\")\n","# axs[1, 0].plot(train_acc)\n","# axs[1, 0].set_title(\"Training Accuracy\")\n","# axs[0, 1].plot(test_losses)\n","# axs[0, 1].set_title(\"Test Loss\")\n","# axs[1, 1].plot(test_acc)\n","# axs[1, 1].set_title(\"Test Accuracy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4gtYupNvG5Yb","colab_type":"code","colab":{}},"source":["# # function boundaries\n","# xmin, xmax, xstep = -4.5, 4.5, .9\n","# ymin, ymax, ystep = -4.5, 4.5, .9\n","\n","\n","# # Let's create some points\n","# x1, y1 = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bcWB1LRH8ADK","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZUd3qZ7777Pm","colab_type":"code","colab":{}},"source":["np.mean(np.random.uniform(low=0.0, high=10.0, size=20))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nF3wxOEX8Ezr","colab_type":"code","colab":{}},"source":["logger.info(f\"\\n===================> L1 - Results of Coarse/finer grid search in various ranges - {para_grid_lambda}<===================\\n\")\n","for final_best_train_acc, final_best_test_acc, final_best_lambda_l1, final_best_lambda_l2 in results_lambda_l1:\n","  logger.info(f\"L1 reg parameter: {final_best_lambda_l1}, L2 reg parameter: {final_best_lambda_l2}, Train_acc: {final_best_train_acc}, Test_acc: {final_best_test_acc}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vkiEVmjdvvM1","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}